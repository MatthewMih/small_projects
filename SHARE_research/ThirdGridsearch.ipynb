{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "358927df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from research_task_utils import *\n",
    "import importlib\n",
    "from torchsummary import summary\n",
    "import numpy as np\n",
    "# importlib.reload(research_task_utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "238ad9f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "batchsize = 512\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "transform = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.ToTensor()\n",
    "    ]\n",
    ")\n",
    "traindata = torchvision.datasets.CIFAR100(root=\"./dataset\", train=True, download=True, transform=transform)\n",
    "testdata = torchvision.datasets.CIFAR100(root=\"./dataset\", train=False, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(traindata, batch_size=batchsize, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(testdata, batch_size=batchsize, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d9f24af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "class ConvBlock(torch.nn.Module):\n",
    "    def __init__(self, num_of_convs, c_in, c_out, wh):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_of_convs = num_of_convs\n",
    "        self.c_in = c_in\n",
    "        self.c_out = c_out\n",
    "        self.wh = wh\n",
    "        \n",
    "        layers = []\n",
    "        \n",
    "        assert num_of_convs > 0\n",
    "        layers.append(('conv1',torch.nn.Conv2d(c_in, c_out, kernel_size=3, stride=1, padding=1)))\n",
    "        for k in range(1, num_of_convs):\n",
    "            layers.append((f'relu_{k}',torch.nn.ReLU()))\n",
    "            layers.append((f'conv_{k+1}',torch.nn.Conv2d(c_out, c_out, kernel_size=3, stride=1, padding=1)))\n",
    "            \n",
    "        layers.append(('bn',torch.nn.BatchNorm2d(c_out)))\n",
    "        layers.append((f'relu_{num_of_convs}',torch.nn.ReLU()))\n",
    "        \n",
    "        self.layers = torch.nn.Sequential(OrderedDict(layers))\n",
    "        \n",
    "    def num_of_params(self, only_trainable: bool = False):\n",
    "        parameters = list(self.parameters())\n",
    "        if only_trainable:\n",
    "            parameters = [p for p in parameters if p.requires_grad]\n",
    "        unique = {p.data_ptr(): p for p in parameters}.values()\n",
    "        return sum(p.numel() for p in unique)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "def num_of_convBlock_params(num_of_convs, c_in, c_out, wh):\n",
    "    num = c_in*c_out*9 + c_out # first conv layer\n",
    "    num += (num_of_convs - 1)*(c_out*c_out*9 + c_out) # other conv layers\n",
    "    num += 2*c_out # BN params\n",
    "    return num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "61b57afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class architecture2():\n",
    "    def __init__(self, c1, c2, c3, c4, k1, k2, k3, k4):\n",
    "        self.c1 = c1\n",
    "        self.c2 = c2\n",
    "        self.c3 = c3\n",
    "        self.c4 = c4\n",
    "        self.k1 = k1\n",
    "        self.k2 = k2\n",
    "        self.k3 = k3\n",
    "        self.k4 = k4\n",
    "        \n",
    "        self.num_of_params = (num_of_convBlock_params(num_of_convs=k1, c_in=3, c_out=c1, wh=32) +\n",
    "                              num_of_convBlock_params(num_of_convs=k2, c_in=c1, c_out=c2, wh=16) +\n",
    "                              num_of_convBlock_params(num_of_convs=k3, c_in=c2, c_out=c3, wh=8) +\n",
    "                              num_of_convBlock_params(num_of_convs=k4, c_in=c3, c_out=c4, wh=4) +\n",
    "                              c4*100+100)\n",
    "        \n",
    "    def get_num_of_params(self):\n",
    "        return self.num_of_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9ecf0f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvArchitecture2(torch.nn.Module):\n",
    "    def __init__(self, model_hyperparams):\n",
    "        super().__init__()\n",
    "        \n",
    "        c1, c2, c3, c4, k1, k2, k3, k4 = model_hyperparams\n",
    "        self.hyperparams = model_hyperparams\n",
    "        \n",
    "        self.layers = torch.nn.Sequential(\n",
    "            ConvBlock(num_of_convs=k1, c_in=3, c_out=c1, wh=32),\n",
    "            torch.nn.MaxPool2d(kernel_size=2),\n",
    "            \n",
    "            ConvBlock(num_of_convs=k2, c_in=c1, c_out=c2, wh=16),\n",
    "            torch.nn.MaxPool2d(kernel_size=2),\n",
    "            \n",
    "            ConvBlock(num_of_convs=k3, c_in=c2, c_out=c3, wh=8),\n",
    "            torch.nn.MaxPool2d(kernel_size=2),\n",
    "            \n",
    "            ConvBlock(num_of_convs=k4, c_in=c3, c_out=c4, wh=4),\n",
    "            \n",
    "            torch.nn.AvgPool2d(kernel_size=4),\n",
    "            torch.nn.Flatten(),\n",
    "            torch.nn.Linear(c4, 100),\n",
    "            torch.nn.Softmax(dim=-1)\n",
    "        )\n",
    "    \n",
    "    def num_of_params(self, only_trainable: bool = False):\n",
    "        parameters = list(self.parameters())\n",
    "        if only_trainable:\n",
    "            parameters = [p for p in parameters if p.requires_grad]\n",
    "        unique = {p.data_ptr(): p for p in parameters}.values()\n",
    "        return sum(p.numel() for p in unique)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7c3e4773",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "import json\n",
    "import os\n",
    "\n",
    "def add_statistics_to_json(model_info, statistics_dict, path):\n",
    "    if not os.path.exists(path):\n",
    "        with open(path, 'w') as f:\n",
    "            json.dump([{\"model_info\": model_info, \"statistics\": statistics_dict}], f)\n",
    "    else:\n",
    "        with open(path) as f:\n",
    "            old_data = json.load(f)\n",
    "        with open(path, 'w') as f:\n",
    "            json.dump(old_data+[{\"model_info\": model_info, \"statistics\": statistics_dict}], f)\n",
    "    \n",
    "\n",
    "def perform_experiment(device, trainloader, testloader, Model_class, model_hyperparams, epochs_n, repeats_n, path):\n",
    "    train_accs = []\n",
    "    test_accs = []\n",
    "    \n",
    "    for k in range(repeats_n):\n",
    "        model = Model_class(model_hyperparams).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters())\n",
    "        loss_function = torch.nn.CrossEntropyLoss()\n",
    "        statistics_list = init_statistics(model, device, trainloader, testloader, loss_function)\n",
    "        train(model, device, optimizer, loss_function, trainloader, testloader, statistics_list, epochs_n=epochs_n)\n",
    "        train_accs.append(statistics_list[\"trainacc\"][-1])\n",
    "        test_accs.append(statistics_list[\"testacc\"][-1])\n",
    "        \n",
    "        print(\"=================\")\n",
    "        print(\"Architecture: \", model_hyperparams)\n",
    "        print(\"test acc:\", statistics_list['testacc'][-1])\n",
    "        plot_statistics(statistics_list)\n",
    "        add_statistics_to_json(model_info={\"name\": \"architecture2\", \"configuration\": model_hyperparams,\n",
    "                                           \"comment\": \"third gridsearch with grads, 100 epochs\"},\n",
    "                               statistics_dict=statistics_list,path=path)\n",
    "        print(\"=================\")\n",
    "        \n",
    "    if repeats_n != 1:\n",
    "        test_accs_std = statistics.stdev(test_accs)\n",
    "        train_accs_std = statistics.stdev(train_accs)\n",
    "    else:\n",
    "        test_accs_std, train_accs_std = 0, 0\n",
    "        \n",
    "    result = {\"train_acc_mean\": statistics.mean(train_accs),\n",
    "              \"train_acc_std\": train_accs_std,\n",
    "              \"test_acc_mean\": statistics.mean(test_accs),\n",
    "              \"test_acc_std\": test_accs_std,\n",
    "              \"num_of_params\": model.num_of_params()\n",
    "             }\n",
    "    if repeats_n == 1:\n",
    "        result[\"statistics\"] = statistics_list\n",
    "    return result\n",
    "\n",
    "\n",
    "def perform_experiments(device, trainloader, testloader, Model_class, hyperparams_list, epochs_n, repeats_n, path):\n",
    "    results = dict()\n",
    "    for hyperparam in hyperparams_list:\n",
    "        results[hyperparam] = perform_experiment(device, trainloader, testloader,\n",
    "                                                   Model_class, hyperparam, epochs_n, repeats_n, path)\n",
    "    return results\n",
    "\n",
    "    \n",
    "def get_N(hyperparams, data_to_compute_gradients):\n",
    "    return data_to_compute_gradients[hyperparams][0]\n",
    "\n",
    "def get_acc(hyperparams, data_to_compute_gradients):\n",
    "    return data_to_compute_gradients[hyperparams][1]\n",
    "\n",
    "def get_grad_N(hyperparams, data_to_compute_gradients):\n",
    "    grad = []\n",
    "    for i in range(len(hyperparams)):\n",
    "        hyperparams_tmp = list(hyperparams)\n",
    "        hyperparams_tmp[i] += 1\n",
    "        grad.append(get_N(tuple(hyperparams_tmp), data_to_compute_gradients) \n",
    "                    - get_N(hyperparams, data_to_compute_gradients))\n",
    "    return np.array(grad)\n",
    "\n",
    "def get_grad_acc(hyperparams, data_to_compute_gradients):\n",
    "    grad = []\n",
    "    for i in range(len(hyperparams)):\n",
    "        hyperparams_tmp = list(hyperparams)\n",
    "        hyperparams_tmp[i] += 1\n",
    "        grad.append(get_acc(tuple(hyperparams_tmp), data_to_compute_gradients) \n",
    "                    - get_acc(hyperparams, data_to_compute_gradients))\n",
    "    return np.array(grad)\n",
    "\n",
    "def generate_architectures_to_check(initial_hyperparams):\n",
    "    architectures = [initial_hyperparams]\n",
    "    for i in range(len(initial_hyperparams)):\n",
    "        hyperparams_tmp = list(initial_hyperparams)\n",
    "        hyperparams_tmp[i] += 1\n",
    "        architectures.append(tuple(hyperparams_tmp))\n",
    "    return architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "edf6bbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # замерджить файлы с колаба и с пк\n",
    "\n",
    "# with open(\"models_logs_gradsearch.json\") as f:\n",
    "#     data_pc = json.load(f)\n",
    "# with open(\"models_logs_gradsearch_colab.json\") as f:\n",
    "#     data_colab = json.load(f)\n",
    "# with open(\"models_logs_gradsearch_merged.json\", 'w') as f:\n",
    "#     json.dump(data_pc + list(reversed(data_colab)), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfde425",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                         | 0/1 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "initial_hyperparams = (8, 8, 16, 32, 4, 4, 4, 4) #c1, c2, c3, c4, k1, k2, k3, k4\n",
    "path = \"models_logs_gradsearch3_test.json\"\n",
    "K = 5\n",
    "while (True):\n",
    "    architectures = generate_architectures_to_check(initial_hyperparams)\n",
    "    results = perform_experiments(device, trainloader, testloader,\n",
    "                                  ConvArchitecture2, architectures, epochs_n=1, repeats_n=1, path=path)\n",
    "    \n",
    "    \n",
    "    with open(path) as f:\n",
    "        logs = json.load(f)\n",
    "        \n",
    "    data_to_compute_gradients = {}\n",
    "    for log in logs:\n",
    "        configuration = log[\"model_info\"][\"configuration\"]\n",
    "        c1, c2, c3, c4, k1, k2, k3, k4 = tuple(configuration)\n",
    "        num_of_params = architecture2(c1, c2, c3, c4, k1, k2, k3, k4).get_num_of_params()\n",
    "        acc_avg = statistics.mean(log[\"statistics\"][\"testacc\"][-10:])\n",
    "        data_to_compute_gradients[tuple(configuration)] =  (num_of_params, acc_avg)\n",
    "        \n",
    "    grad_N = get_grad_N(initial_hyperparams, data_to_compute_gradients)\n",
    "    grad_acc = get_grad_acc(initial_hyperparams, data_to_compute_gradients)\n",
    "\n",
    "    direction_to_change_params = grad_acc - np.dot(grad_N, grad_acc) / np.dot(grad_N, grad_N) * grad_N\n",
    "    direction_to_change_params /= np.linalg.norm(direction_to_change_params)\n",
    "        \n",
    "    new_hyperparams = np.array(initial_hyperparams) + K*direction_to_change_params\n",
    "    new_hyperparams = np.rint(new_hyperparams).astype(int)\n",
    "    \n",
    "    for i in range(8):\n",
    "        if new_hyperparams[i] < 1:\n",
    "            new_hyperparams[i] = 1\n",
    "    \n",
    "    initial_hyperparams = tuple(new_hyperparams)\n",
    "    print(\"NEW INITIAL PARAMS: \", initial_hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be64523a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
